# 머신러닝 모델의 학습과 평가

## 머신러닝 모델 최적화의 기초

### 목표와 개요

```{image} figs/image-3-1-1.jpeg
:width: 90%
:align: center
```

- 머신러닝 모델 최적화는 모델의 예측 성능을 극대화하는 과정입니다.
- 이 과정을 통해 데이터의 패턴을 더 잘 이해하고, 실제 세계의 복잡한 문제를 해결할 수 있습니다.

### 선형 회귀 모델의 기본 개념

```{image} figs/image-3-1-2.jpeg
:width: 90%
:align: center
```

- 선형 회귀 모델은 가장 기본적인 머신러닝 모델 중 하나입니다.
- 이 모델은 데이터의 관계를 y = ax + b 형태의 직선으로 나타냅니다.
  - 여기서 y는 종속 변수, x는 독립 변수, a는 기울기, b는 y절편입니다.
- 선형 회귀는 데이터 포인트들과 가장 잘 맞는 직선을 찾는 과정입니다.

### 최소 제곱법 소개

```{image} figs/image-3-1-3.jpeg
:width: 90%
:align: center
```

- 최소 제곱법은 선형 회귀 모델에서 매개변수 a와 b를 찾는 표준 방법입니다.
- 이 방법은 데이터 포인트와 선형 모델 간의 거리(오차)의 제곱을 최소화합니다.
- 수학적으로는 오차의 제곱 합을 최소화하는 a와 b를 찾는 것을 의미합니다.

### 평균 제곱 오차(MSE) 이해

```{image} figs/image-3-1-4.jpeg
:width: 90%
:align: center
```

- 평균 제곱 오차(MSE)는 모델의 예측값과 실제값 간의 차이를 수치화하는 방법입니다.
- MSE = (1/n) Σ(actual - prediction)²
  - 여기서 n은 샘플의 수, actual은 실제값, prediction은 예측값입니다.
- MSE가 작을수록 모델의 예측 성능이 좋다고 할 수 있습니다.
- MSE는 모델의 성능을 평가하고 비교하는 데 중요한 지표로 사용됩니다.

## 비용함수와 손실함수

- **비용함수와 손실함수의 정의**
  - 모델의 예측이 얼마나 잘못되었는지 나타내는 함수.
- **평균제곱오차의 역할**
  - MSE는 모델의 예측 오차를 측정하는 데 사용됩니다.
- **오차 함수와 목적 함수의 비교**
  - 오차 함수는 개별 예측의 오류를, 목적 함수는 전체 데이터셋에 대한 모델의 성능을 평가.

## 경사하강법 (Gradient Descent)

- **경사하강법의 기본 원리**
  - 비용함수의 최소점을 찾기 위해 기울기(gradient)를 사용하는 방법.
- **경사하강법의 다양한 변형**
  - 다양한 변형 방법(SGD, Momentum 등)과 각각의 특징 및 적용 상황.
- **경사하강법 시각화**
  - 비용함수의 모양과 경사하강법의 경로를 시각적으로 이해하기.

## 머신러닝 모델의 일반화

- **과대적합과 과소적합**
  - 모델이 학습 데이터에 지나치게 적합되거나, 충분히 학습되지 않는 현상.
- **최적화 알고리즘의 중요성**
  - 모델의 일반화 능력을 향상시키는 최적화 알고리즘.
- **과대적합과 과소적합 방지 전략**
  - 데이터 증강, 교차 검증, 정규화 기법 등의 전략 소개.

## 좋은 머신러닝 모델의 조건

- **데이터의 양과 질의 중요성**
  - 충분하고 다양한, 고품질의 데이터의 중요성 강조.
- **모델 복잡도 조절**
  - PCA 등을 사용하여 모델의 복잡도 및 특성 수를 조절하는 방법.
- **일반화, 정규화, 가중치 규제**
  - 모델의 과적합을 방지하고 일반화 성능을 향상시키는 기법들 소개.
