
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>언어 모델 &#8212; AI 세상 속으로</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../_static/book.css?v=b36e1efd" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=afe5de03"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week10/session3';</script>
    <link rel="icon" href="../_static/chu-favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Week 11: 생성형 AI" href="../week11/index.html" />
    <link rel="prev" title="자연어처리 기법" href="session2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">AI 세상 속으로</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    인공지능, 머신러닝, 딥러닝 그게 뭐야?
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../week01/index.html">Week 1: 인공지능 소개</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../week01/session1.html">인공지능의 역사</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week01/session2.html">인공지능의 유형들</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week01/session3.html">인공지능 문제 해결 및 탐색전략</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week05/index.html">Week 5: 기계학습</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../week05/session1.html">머신러닝의 개요</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week05/session2.html">머신러닝 학습 방법</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week05/session3.html">머신러닝 모델의 학습과 평가</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week05/lab.html">머신러닝 알고리즘 실습</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week06/index.html">Week 6: 추천시스템</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../week06/session1.html">추천 시스템의 기본 개념</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week06/session2.html">추천 시스템의 고급 주제</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week06/lab.html">추천 시스템 실습</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Week 10: 자연어처리</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="session1.html">자연어처리의 이해</a></li>
<li class="toctree-l2"><a class="reference internal" href="session2.html">자연어처리 기법</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">언어 모델</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week11/index.html">Week 11: 생성형 AI</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../week11/session1.html">생성형 AI 입문</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week11/session2.html">멀티모달 생성 인공지능</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week11/session3.html">AI 에이전트</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/chu-aie/aibasics" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/chu-aie/aibasics/edit/main/book/week10/session3.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/chu-aie/aibasics/issues/new?title=Issue%20on%20page%20%2Fweek10/session3.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/week10/session3.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>언어 모델</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">언어 모델의 개요</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">일반 개념</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">언어 모델이 필요한 이유</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#n-gram">N-gram 언어 모델</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">일반 개념</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">N-gram 모델의 장점 및 한계</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">N-gram과 확률 추정</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bigram">Bigram 모델</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">언어 모델로부터 문장 샘플링</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plms">사전 훈련된 언어 모델(PLMs)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">문맥화 단어 임베딩</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">사전 훈련된 언어 모델의 특성</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">분류</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">훈련 목표 상세</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">Transformer 아키텍처의 적용</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llms">대규모 언어 모델(LLMs) 개요</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">정의 및 특징</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">LLMs의 능력</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">방법론</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">응용 분야</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">대규모 언어 모델의 이슈</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">대규모 언어 모델의 능력과 진화</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">대규모 언어 모델의 진화</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">언어 모델링의 발전</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">대규모 언어 모델의 우위</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">기반 모델: 확장 또는 하위 집합?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">대규모 언어 모델의 아키텍처 진화</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn">초기 프레임워크: 자기지도 학습 &amp; RNN</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-lstm-gru">RNN, LSTM, GRU의 제약사항</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">주의 메커니즘의 도입</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">단어 임베딩과 트랜스포머의 등장</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">인코더-디코더 프레임워크</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">대규모 언어 모델 활용 방법</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">미세 조정을 통한 전이 학습 전략</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">프롬프트 엔지니어링 및 맥락 학습</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">명령 튜닝을 통한 제로샷 프롬프트</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id31">대규모 언어 모델의 신흥 능력</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>언어 모델<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<section id="id2">
<h2>언어 모델의 개요<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<section id="id3">
<h3>일반 개념<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-1-11.jpeg"><img alt="../_images/image-3-1-11.jpeg" class="align-center" src="../_images/image-3-1-11.jpeg" style="width: 90%;" /></a>
<ul class="simple">
<li><p><strong>언어 모델 정의</strong>: 언어 모델은 단어 시퀀스에 확률을 할당하거나 시퀀스에서 다음 단어를 예측하는 계산 모델.</p></li>
<li><p><strong>적용 분야</strong>: 음성 인식, 기계 번역, 맞춤법 및 문법 수정, 텍스트 생성 등 다양한 자연어 처리 작업에서 중요한 역할.</p></li>
<li><p><strong>모델 유형</strong>: N-gram 모델(단어 시퀀스에 의존)과 신경 언어 모델(딥러닝 기술을 활용하여 텍스트를 이해하고 생성) 등.</p></li>
<li><p><strong>중요성</strong>: 언어 모델은 언어의 구조와 패턴을 포착하여 특정 맥락에서 주어진 단어나 구가 나타날 확률을 추정할 수 있음.</p></li>
<li><p><strong>개발 필요성</strong>: 자연어 처리 시스템의 성능을 개선하기 위해서는 효과적인 언어 모델의 이해와 개발이 필수적.</p></li>
</ul>
</section>
<section id="id4">
<h3>언어 모델이 필요한 이유<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-1-22.jpeg"><img alt="../_images/image-3-1-22.jpeg" class="align-center" src="../_images/image-3-1-22.jpeg" style="width: 90%;" /></a>
<ul class="simple">
<li><p><strong>모호성 해결</strong>: 언어 모델은 음성 인식과 텍스트 처리에서 다양한 해석에 확률을 할당하고, 맥락을 바탕으로 가장 가능성 높은 해석을 선택하는 데 도움 제공.</p></li>
<li><p><strong>기계 번역</strong>: 언어 간 텍스트 번역 시, 언어 모델은 목표 언어에서 단어 시퀀스의 유창성과 정확성을 선택하는 데 도움 제공.</p></li>
<li><p><strong>텍스트 생성</strong>: 언어 모델은 요약, 질문 응답, 대화 시스템과 같은 작업을 위한 일관성 있고 문맥적으로 관련된 텍스트를 생성할 수 있음.</p></li>
<li><p><strong>맞춤법 및 문법 수정</strong>: 언어 모델은 다양한 단어 시퀀스의 확률을 비교하여 오류를 식별하고 더 가능성 있는 대안을 제안하여 오류를 수정할 수 있음.</p></li>
<li><p><strong>보조 기술</strong>: 언어 장애가 있는 사용자를 위한 대체 및 보완 커뮤니케이션(AAC) 시스템에서 언어 모델은 사용자에게 가장 가능성이 높은 단어나 구를 예측하고 제안하여 효율적인 커뮤니케이션을 가능하게 함.</p></li>
</ul>
</section>
</section>
<section id="n-gram">
<h2>N-gram 언어 모델<a class="headerlink" href="#n-gram" title="Link to this heading">#</a></h2>
<section id="id5">
<h3>일반 개념<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-2-12.jpeg"><img alt="../_images/image-3-2-12.jpeg" class="align-center" src="../_images/image-3-2-12.jpeg" style="width: 90%;" /></a>
<ul class="simple">
<li><p><strong>N-gram 언어 모델 정의</strong>: N-gram 언어 모델은 시퀀스에서 다음 단어를 (n-1)개의 이전 단어에 기반하여 예측하는 접근법입니다.</p></li>
<li><p><strong>N-gram 유형</strong>:</p>
<ul>
<li><p><strong>Unigram (단일어)</strong>: 단어 하나만 고려하며, 맥락을 무시합니다 (n=1).</p></li>
<li><p><strong>Bigram (이중어)</strong>: 두 단어 시퀀스를 고려합니다 (n=2).</p></li>
<li><p><strong>Trigram (삼중어)</strong>: 세 단어 시퀀스를 고려합니다 (n=3).</p></li>
<li><p><strong>고차 N-gram</strong>: 더 긴 단어 시퀀스를 고려합니다 (n&gt;3).</p></li>
</ul>
</li>
<li><p><strong>마르코프 가정</strong>: 단어의 확률은 오직 이전의 (n-1)개 단어에만 의존합니다.</p></li>
<li><p><strong>확률 추정</strong>: 큰 말뭉치에서 n-gram의 출현 횟수를 세고 정규화하여 확률을 추정합니다.</p></li>
</ul>
</section>
<section id="id6">
<h3>N-gram 모델의 장점 및 한계<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>장점</p>
<ul>
<li><p>구현이 간단하고 효율적입니다.</p></li>
<li><p>계산 및 메모리 사용 측면에서 효율적입니다.</p></li>
</ul>
</li>
<li><p>한계</p>
<ul>
<li><p>단어 간 장기 의존성을 포착하지 못합니다.</p></li>
<li><p>훈련 말뭉치에 나타나지 않은 n-gram에 대해 데이터 희소성 문제에 민감합니다.</p></li>
</ul>
</li>
</ul>
</section>
<section id="id7">
<h3>N-gram과 확률 추정<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-2-32.jpeg"><img alt="../_images/image-3-2-32.jpeg" class="align-center" src="../_images/image-3-2-32.jpeg" style="width: 90%;" /></a>
<ul>
<li><p><strong>확률 추정</strong>: 맥락 h가 주어졌을 때 단어 w의 확률 P(w|h)을 추정할 수 있습니다.</p></li>
<li><p><strong>확률 추정 방법</strong>: 큰 말뭉치에서 맥락 뒤에 타겟 단어가 나타나는 빈도를 세고 맥락의 총 빈도로 나누어 확률을 추정합니다.</p>
<div class="math notranslate nohighlight">
\[
  P(\text{pizza}|\text{I like to eat}) = \frac{C(\text{I like to eat pizza})}{C(\text{I like to eat})}
  \]</div>
<div class="math notranslate nohighlight">
\[
  P(\text{먹습니다}|\text{저는 김치를}) = \frac{C(\text{저는 김치를 먹습니다})}{C(\text{저는 김치를})}
  \]</div>
</li>
</ul>
<section id="bigram">
<h4>Bigram 모델<a class="headerlink" href="#bigram" title="Link to this heading">#</a></h4>
<ul>
<li><p>전체 맥락을 고려하는 대신 이전 단어만을 고려하여 확률을 추정합니다.</p></li>
<li><p>이러한 단순화는 확률을 더 신뢰할 수 있게 추정할 수 있게 하지만, 더 긴 맥락 의존성을 포착하지 못할 수 있습니다.</p>
<div class="math notranslate nohighlight">
\[
  P(\text{pizza}|\text{I like to eat}) \approx P(\text{pizza}|\text{eat})
  \]</div>
<div class="math notranslate nohighlight">
\[
  P(\text{먹습니다}|\text{저는 김치를}) \approx P(\text{먹습니다}|\text{김치를})
  \]</div>
</li>
</ul>
</section>
</section>
<section id="id8">
<h3>언어 모델로부터 문장 샘플링<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-2-51.jpeg"><img alt="../_images/image-3-2-51.jpeg" class="align-center" src="../_images/image-3-2-51.jpeg" style="width: 90%;" /></a>
<ul class="simple">
<li><p><strong>문장 샘플링</strong>: 언어 모델로부터 정의된 확률에 따라 문장을 생성하는 방법입니다.</p></li>
<li><p><strong>과정</strong>: 더 높은 확률을 가진 문장은 낮은 확률을 가진 문장보다 생성될 가능성이 더 높습니다.</p></li>
<li><p><strong>예시</strong>: Unigram 언어 모델에서는 모델의 어휘에 있는 모든 단어의 확률 분포를 시각화할 수 있으며, Bigram 모델에서는 첫 단어가 <code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span></code>로 시작하는 랜덤 bigram을 생성합니다.</p></li>
</ul>
</section>
</section>
<section id="plms">
<h2>사전 훈련된 언어 모델(PLMs)<a class="headerlink" href="#plms" title="Link to this heading">#</a></h2>
<section id="id9">
<h3>문맥화 단어 임베딩<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-5-12.jpeg"><img alt="../_images/image-3-5-12.jpeg" class="align-center" src="../_images/image-3-5-12.jpeg" style="width: 90%;" /></a>
<ul class="simple">
<li><p><strong>정의</strong>: 문장의 맥락에 따라 단어의 의미를 파악하는 단어 임베딩 방식.</p></li>
<li><p><strong>정적 임베딩</strong>: Word2Vec, GloVe와 같이 문맥을 고려하지 않는 임베딩.</p></li>
<li><p><strong>동적 임베딩</strong>: ELMo, BERT와 같이 문맥을 반영하여 단어의 의미를 파악하는 모델.</p></li>
</ul>
</section>
<section id="id10">
<h3>사전 훈련된 언어 모델의 특성<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>목적</strong>: 대규모 텍스트 데이터를 통해 언어의 일반적 구조를 학습.</p></li>
<li><p><strong>자기지도 학습</strong>: 레이블 없는 데이터를 이용한 사전 훈련 방식.</p></li>
<li><p><strong>가중치 활용</strong>: 다양한 NLP 작업에 모델을 미세 조정하기 위한 기반 제공.</p></li>
</ul>
</section>
<section id="id11">
<h3>분류<a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-5-22.jpeg"><img alt="../_images/image-3-5-22.jpeg" class="align-center" src="../_images/image-3-5-22.jpeg" style="width: 90%;" /></a>
<ul class="simple">
<li><p><strong>훈련 목표별 분류</strong></p>
<ul>
<li><p>표준 언어 모델: 다음 단어 예측.</p></li>
<li><p>마스크된 언어 모델: 누락된 단어 예측.</p></li>
<li><p>순서 변경 언어 모델: 임의 순서로 단어 예측.</p></li>
</ul>
</li>
<li><p><strong>아키텍처별 분류</strong></p>
<ul>
<li><p>인코더-오직 모델: 모든 다른 토큰을 기반으로 토큰 예측.</p></li>
<li><p>디코더-오직 모델: 이전 토큰을 기반으로 다음 토큰 예측.</p></li>
<li><p>인코더-디코더 모델: 입력 시퀀스로부터 출력 시퀀스 생성.</p></li>
</ul>
</li>
</ul>
</section>
<section id="id12">
<h3>훈련 목표 상세<a class="headerlink" href="#id12" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>표준 자기회귀 언어 모델링</strong></p>
<ul>
<li><p>RNN, LSTM 사용: 문맥 전체 이해의 한계.</p></li>
</ul>
</li>
<li><p><strong>마스크된 언어 모델링</strong></p>
<ul>
<li><p>BERT의 대표적 사용: 양방향 문맥 이해.</p></li>
</ul>
</li>
<li><p><strong>인과적 마스크된 언어 모델링</strong></p>
<ul>
<li><p>GPT의 접근: 마스크된 단어 간 의존성 고려.</p></li>
</ul>
</li>
<li><p><strong>순서 변경 언어 모델링</strong></p>
<ul>
<li><p>XLNet의 특징: 시퀀스 내 모든 토큰 조합 간 양방향 의존성 학습.</p></li>
</ul>
</li>
</ul>
</section>
<section id="transformer">
<h3>Transformer 아키텍처의 적용<a class="headerlink" href="#transformer" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-5-32.jpeg"><img alt="../_images/image-3-5-32.jpeg" class="align-center" src="../_images/image-3-5-32.jpeg" style="width: 90%;" /></a>
<ul class="simple">
<li><p><strong>구성 요소</strong>: 인코더와 디코더의 조합.</p></li>
<li><p><strong>인코더</strong>: 입력 시퀀스를 벡터 표현으로 변환.</p></li>
<li><p><strong>디코더</strong>: 벡터 표현을 출력 시퀀스로 변환.</p></li>
<li><p><strong>PLM 적용</strong>: 다양한 NLP 작업에 대한 유연한 적용 가능성.</p></li>
</ul>
</section>
</section>
<section id="llms">
<h2>대규모 언어 모델(LLMs) 개요<a class="headerlink" href="#llms" title="Link to this heading">#</a></h2>
<section id="id13">
<h3>정의 및 특징<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-3-12.jpeg"><img alt="../_images/image-3-3-12.jpeg" class="align-center" src="../_images/image-3-3-12.jpeg" style="width: 90%;" /></a>
<ul class="simple">
<li><p><strong>대규모 언어 모델(Large Language Models, LLMs) 정의</strong></p>
<ul>
<li><p>자연어 처리(NLP) 작업을 위한 심층 학습 모델</p></li>
<li><p>방대한 텍스트 데이터 학습을 통한 언어 패턴, 구조, 의미 관계 이해 및 생성 능력</p></li>
</ul>
</li>
<li><p><strong>주요 특징</strong></p>
<ul>
<li><p><strong>아키텍처</strong>: Transformer 기반, 자기 주의 메커니즘 사용</p></li>
<li><p><strong>사전 훈련(Pre-training)</strong>: 대규모 데이터셋 비지도 학습</p></li>
<li><p><strong>세분화 훈련(Fine-tuning)</strong>: 특정 작업/도메인 레이블 데이터셋에서의 특화</p></li>
<li><p><strong>전이 학습(Transfer Learning)</strong>: 다양한 NLP 작업에 대한 지식 활용</p></li>
</ul>
</li>
<li><p><strong>예시 모델</strong></p>
<ul>
<li><p>GPT-3, BERT, RoBERTa, T5 등</p></li>
</ul>
</li>
</ul>
</section>
<section id="id14">
<h3>LLMs의 능력<a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-3-22.jpeg"><img alt="../_images/image-3-3-22.jpeg" class="align-center" src="../_images/image-3-3-22.jpeg" style="width: 90%;" /></a>
<ul class="simple">
<li><p><strong>언어 이해</strong>: 다양한 맥락에서의 텍스트 처리 및 해석</p></li>
<li><p><strong>텍스트 생성</strong>: 입력 기반 맥락상 관련성 높은 텍스트 생성</p></li>
<li><p><strong>질문 응답</strong>: 정보 이해 및 추출을 통한 구체적 질문 답변</p></li>
<li><p><strong>번역</strong>: 다양한 언어 간 텍스트 번역</p></li>
<li><p><strong>제로샷 및 퓨샷 학습</strong>: 레이블 없이 또는 소수 예시로 작업 수행</p></li>
</ul>
</section>
<section id="id15">
<h3>방법론<a class="headerlink" href="#id15" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-3-32.jpeg"><img alt="../_images/image-3-3-32.jpeg" class="align-center" src="../_images/image-3-3-32.jpeg" style="width: 90%;" /></a>
<ul class="simple">
<li><p><strong>아키텍처 디자인</strong>: 고급 신경망 아키텍처 사용</p></li>
<li><p><strong>사전 훈련</strong>: 대규모 데이터셋에서 비지도 학습</p></li>
<li><p><strong>세분화 훈련</strong>: 작업 특화를 위한 레이블 데이터셋 훈련</p></li>
<li><p><strong>전이 학습</strong>: 사전 훈련된 지식을 다양한 작업에 적용</p></li>
<li><p><strong>프롬프트 엔지니어링</strong>: 입력 설계 및 수정을 통한 출력 개선</p></li>
</ul>
</section>
<section id="id16">
<h3>응용 분야<a class="headerlink" href="#id16" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-3-41.jpeg"><img alt="../_images/image-3-3-41.jpeg" class="align-center" src="../_images/image-3-3-41.jpeg" style="width: 90%;" /></a>
<ul class="simple">
<li><p><strong>기계 번역</strong>: 언어 장벽 해소</p></li>
<li><p><strong>감성 분석</strong>: 텍스트의 긍정적, 부정적, 중립적 분류</p></li>
<li><p><strong>텍스트 요약</strong>: 긴 문서나 기사의 핵심 정보 요약</p></li>
<li><p><strong>질문 응답 시스템</strong>: 가상 비서, 고객 지원 챗봇 활용</p></li>
<li><p><strong>텍스트 생성 및 콘텐츠 생성</strong>: 저널리즘, 마케팅, 창의적 글쓰기 등에 활용</p></li>
</ul>
</section>
<section id="id17">
<h3>대규모 언어 모델의 이슈<a class="headerlink" href="#id17" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-4-12.jpeg"><img alt="../_images/image-3-4-12.jpeg" class="align-center" src="../_images/image-3-4-12.jpeg" style="width: 90%;" /></a>
<ul class="simple">
<li><p><strong>자원 요구량</strong></p>
<ul>
<li><p>훈련에 막대한 계산 자원 필요</p></li>
<li><p>비용 및 에너지 집약적</p></li>
<li><p>AI 연구의 중앙집중화 우려</p></li>
</ul>
</li>
<li><p><strong>데이터 편향</strong></p>
<ul>
<li><p>훈련 데이터셋의 편향성 반영 가능성</p></li>
<li><p>부정적 또는 편향된 출력 결과 초래 가능</p></li>
</ul>
</li>
<li><p><strong>윤리적 고려사항</strong></p>
<ul>
<li><p>인간 글과 구분 불가능한 콘텐츠 생성</p></li>
<li><p>가짜 뉴스, 악의적 콘텐츠 생성에 대한 우려</p></li>
</ul>
</li>
<li><p><strong>모델 해석 가능성</strong></p>
<ul>
<li><p>복잡한 아키텍처와 매개변수로 인한 해석 어려움</p></li>
<li><p>해석성 및 설명 가능성 개선 필요</p></li>
</ul>
</li>
<li><p><strong>배포 과제</strong></p>
<ul>
<li><p>실제 환경에서의 배포에 고도의 계산 자원 및 전문 지식 요구</p></li>
<li><p>접근성 및 배포 과정 단순화 필요</p></li>
</ul>
</li>
<li><p><strong>공정성 및 포괄성</strong></p>
<ul>
<li><p>다양한 언어, 방언, 문화적 관점의 공정한 처리</p></li>
<li><p>소수 언어 및 방언의 대표성 문제</p></li>
</ul>
</li>
<li><p><strong>환경적 영향</strong></p>
<ul>
<li><p>훈련 시 에너지 소비로 인한 환경적 우려</p></li>
<li><p>에너지 효율적인 훈련 방법 및 재생 가능 에너지 사용 촉진 필요</p></li>
</ul>
</li>
<li><p><strong>법적 및 규제적 함의</strong></p>
<ul>
<li><p>지적 재산권, 콘텐츠 조정, 생성물에 대한 책임 등 법적 문제</p></li>
<li><p>책임 있는 혁신을 위한 법적, 규제적 틀 개발 필요</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="id18">
<h2>대규모 언어 모델의 능력과 진화<a class="headerlink" href="#id18" title="Link to this heading">#</a></h2>
<section id="id19">
<h3>대규모 언어 모델의 진화<a class="headerlink" href="#id19" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-4-32.jpeg"><img alt="../_images/image-3-4-32.jpeg" class="align-center" src="../_images/image-3-4-32.jpeg" style="width: 90%;" /></a>
<section id="id20">
<h4>언어 모델링의 발전<a class="headerlink" href="#id20" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>1980년대의 계산력 및 데이터 증가</strong>: 통계 및 기계 학습 기반 NLP 기술의 등장 배경.</p></li>
<li><p><strong>언어 모델링의 핵심</strong>: 단어 시퀀스의 가능성 추정을 위한 확률 모델 활용.</p></li>
<li><p><strong>기술 전환</strong>: n-gram에서 신경망 기반 모델로의 전환, 문맥적 언어 표현 제공.</p></li>
</ul>
</section>
<section id="id21">
<h4>대규모 언어 모델의 우위<a class="headerlink" href="#id21" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>능력과 규모의 도약</strong>: 언어 모델링에서의 중요한 진전.</p></li>
<li><p><strong>훈련 데이터의 광범위함</strong>: 인간 지식의 방대한 영역 포괄.</p></li>
<li><p><strong>신흥 능력의 발현</strong>: 작업 특정 훈련 없이도 다양한 언어 작업 수행.</p></li>
</ul>
</section>
<section id="id22">
<h4>기반 모델: 확장 또는 하위 집합?<a class="headerlink" href="#id22" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>‘기반 모델’ 용어의 도입</strong>: 다양한 작업에 적응 가능한 모델 설명.</p></li>
<li><p><strong>다중 모달 데이터의 포함</strong>: 더 넓은 범위의 데이터 다양성.</p></li>
<li><p><strong>아키텍처 설계</strong>: 작업 특정 적응을 위한 기반층.</p></li>
</ul>
</section>
</section>
<section id="id23">
<h3>대규모 언어 모델의 아키텍처 진화<a class="headerlink" href="#id23" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-4-41.jpeg"><img alt="../_images/image-3-4-41.jpeg" class="align-center" src="../_images/image-3-4-41.jpeg" style="width: 90%;" /></a>
<section id="rnn">
<h4>초기 프레임워크: 자기지도 학습 &amp; RNN<a class="headerlink" href="#rnn" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>자기지도 학습 알고리즘</strong>: 레이블 없는 데이터에서의 특성 표현 학습.</p></li>
<li><p><strong>RNN, LSTM, GRU 사용</strong>: 초기 대규모 언어 모델 아키텍처.</p></li>
</ul>
</section>
<section id="rnn-lstm-gru">
<h4>RNN, LSTM, GRU의 제약사항<a class="headerlink" href="#rnn-lstm-gru" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>순차적 데이터 처리의 어려움</strong>: 병렬화의 한계.</p></li>
<li><p><strong>기울기 소실 문제</strong>: LSTM의 효과적 훈련 방해.</p></li>
<li><p><strong>GRU의 성능 한계</strong>: 방대한 데이터셋에서의 성능 저하.</p></li>
</ul>
</section>
<section id="id24">
<h4>주의 메커니즘의 도입<a class="headerlink" href="#id24" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>RNN의 제한 완화</strong>: 주의 메커니즘을 통한 정보 전파 및 보존 범위 확장.</p></li>
</ul>
</section>
<section id="id25">
<h4>단어 임베딩과 트랜스포머의 등장<a class="headerlink" href="#id25" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>단어 임베딩의 역할</strong>: 언어 맥락과 의미 연관성 포착.</p></li>
<li><p><strong>트랜스포머 아키텍처</strong>: 자기 주의 메커니즘을 사용한 데이터 처리의 혁신.</p></li>
</ul>
</section>
<section id="id26">
<h4>인코더-디코더 프레임워크<a class="headerlink" href="#id26" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>트랜스포머 모델의 구조</strong>: 인코더와 디코더의 결합, 복잡한 정보 합성 및 검색 가능.</p></li>
</ul>
</section>
</section>
<section id="id27">
<h3>대규모 언어 모델 활용 방법<a class="headerlink" href="#id27" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-4-5.jpeg"><img alt="../_images/image-3-4-5.jpeg" class="align-center" src="../_images/image-3-4-5.jpeg" style="width: 90%;" /></a>
<section id="id28">
<h4>미세 조정을 통한 전이 학습 전략<a class="headerlink" href="#id28" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>미세 조정의 개념</strong>: 사전 훈련된 모델의 작업 특정 데이터셋에서의 추가 전문화.</p></li>
</ul>
</section>
<section id="id29">
<h4>프롬프트 엔지니어링 및 맥락 학습<a class="headerlink" href="#id29" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>프롬프트 엔지니어링의 중요성</strong>: 특정 작업에 대한 LLM의 적용을 용이하게 함.</p></li>
</ul>
</section>
<section id="id30">
<h4>명령 튜닝을 통한 제로샷 프롬프트<a class="headerlink" href="#id30" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>명령 튜닝의 방법론</strong>: 더 정확하고 자연스러운 제로샷 프롬프트를 위한 미세 조정 방법.</p></li>
</ul>
</section>
</section>
<section id="id31">
<h3>대규모 언어 모델의 신흥 능력<a class="headerlink" href="#id31" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/image-3-4-22.jpeg"><img alt="../_images/image-3-4-22.jpeg" class="align-center" src="../_images/image-3-4-22.jpeg" style="width: 90%;" /></a>
<ul class="simple">
<li><p><strong>신흥 능력 정의</strong></p>
<ul>
<li><p>작은 모델에서는 나타나지 않지만 큰 모델에서 발견되는 능력</p></li>
</ul>
</li>
<li><p><strong>신흥 능력의 분류</strong></p>
<ul>
<li><p><strong>신흥 Few-shot Prompted Tasks</strong></p>
<ul>
<li><p>작은 모델은 무작위 수준의 성능, 큰 모델은 상당히 높은 성능</p></li>
<li><p>BIG-Bench, Massive Multitask Benchmark 등에서 소개</p></li>
</ul>
</li>
<li><p><strong>신흥 Prompting 전략</strong></p>
<ul>
<li><p>충분히 큰 모델에만 작동하는 일반적인 프롬프팅 전략</p></li>
<li><p>지시 사항 따르기, 스크래치패드, 사고의 연쇄 프롬프팅 등 포함</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>향후 연구 방향</strong></p>
<ul>
<li><p>모델 아키텍처 개선</p></li>
<li><p>데이터의 질 및 양 향상</p></li>
<li><p>프롬프팅 최적화</p></li>
<li><p>신흥 능력의 발생 이유 및 예측 가능성 탐구</p></li>
</ul>
</li>
<li><p><strong>BIG-Bench 및 MMLU 작업 예시</strong></p>
<ul>
<li><p>GPT-3, LaMDA, PaLM, Chinchilla 모델의 다양한 작업 성능 소개</p></li>
<li><p>다양한 언어, 문화적 관점을 포괄하는 과제 포함</p></li>
</ul>
</li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "chu-aie/aibasics",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./week10"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="session2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">자연어처리 기법</p>
      </div>
    </a>
    <a class="right-next"
       href="../week11/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Week 11: 생성형 AI</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">언어 모델의 개요</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">일반 개념</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">언어 모델이 필요한 이유</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#n-gram">N-gram 언어 모델</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">일반 개념</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">N-gram 모델의 장점 및 한계</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">N-gram과 확률 추정</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bigram">Bigram 모델</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">언어 모델로부터 문장 샘플링</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plms">사전 훈련된 언어 모델(PLMs)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">문맥화 단어 임베딩</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">사전 훈련된 언어 모델의 특성</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">분류</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">훈련 목표 상세</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">Transformer 아키텍처의 적용</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llms">대규모 언어 모델(LLMs) 개요</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">정의 및 특징</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">LLMs의 능력</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">방법론</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">응용 분야</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">대규모 언어 모델의 이슈</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">대규모 언어 모델의 능력과 진화</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">대규모 언어 모델의 진화</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">언어 모델링의 발전</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">대규모 언어 모델의 우위</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">기반 모델: 확장 또는 하위 집합?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">대규모 언어 모델의 아키텍처 진화</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn">초기 프레임워크: 자기지도 학습 &amp; RNN</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-lstm-gru">RNN, LSTM, GRU의 제약사항</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">주의 메커니즘의 도입</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">단어 임베딩과 트랜스포머의 등장</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">인코더-디코더 프레임워크</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">대규모 언어 모델 활용 방법</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">미세 조정을 통한 전이 학습 전략</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">프롬프트 엔지니어링 및 맥락 학습</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">명령 튜닝을 통한 제로샷 프롬프트</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id31">대규모 언어 모델의 신흥 능력</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By 김성진, 이영준, 장용준
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>